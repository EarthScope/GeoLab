{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe38ad28-c3c2-48be-946b-5352eae82ded",
   "metadata": {},
   "source": [
    "# Getting Seismic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c2659-11c1-4f59-9903-9681a4bb8434",
   "metadata": {},
   "source": [
    "This section details how to get seismic data from EarthScope. This section requires a working knowledge of Python. You should have an understanding of data types and string formatting, creating file paths and file naming, web requests, and creating Python functions. In addition to Python, you should have an understanding of miniSEED data distributed through the International Federation of Digital Seismograph Networks (FDSN). \n",
    "\n",
    "The notebook material is informational and is useful for completing the seismic and geodetic exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705f359",
   "metadata": {},
   "source": [
    "## Getting Seismic Data from EarthScope\n",
    "\n",
    "Seismic data is available through third party packages such as [obspy](https://docs.obspy.org/) or through EarthScope's `dataselect` web service.\n",
    "\n",
    "![](images/Web_Services_Data_Flow.png)\n",
    "\n",
    "Currently, the simplest way to access EarthScope's seismic data is to use a third party package such as obspy. If you want to work directly with miniSEED files, EarthScope's dataselect service supports selecting sorting event data and returns data in the miniSEED format. \n",
    "\n",
    "![](images/cloud_native_data_access.png)\n",
    "\n",
    "In the future, data will be available from EarthScopes cloud services hosted in Amazon's S3 storage service. We provide an overview of how data is stored in S3 and an example of how to access data from a public S3 bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbff8a",
   "metadata": {},
   "source": [
    "### Getting Data from Obspy and Third-Party Packages\n",
    "\n",
    "Obspy is a Python framework (or package) for processing seismic data. It provides parsers for common file formats, clients to access data centers and seismological signal processing routines which allow the manipulation of seismological time series.\n",
    "\n",
    "We'll start by importing modules from the obspy package. It's not necessary to import the entire package, just the function from the module, e.g., obspy.clients.fdsn. Next we create a client that connects to EarthScope's services and send a query based on start and end time, and the minimum magnitude of an event.\n",
    "\n",
    "```python\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.fdsn.header import URL_MAPPINGS\n",
    "from obspy import UTCDateTime\n",
    "import warnings\n",
    "import cartopy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# creates a client that connects to the IRIS data center\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "starttime = UTCDateTime(\"2020-01-01\")\n",
    "endtime = UTCDateTime(\"2025-12-31\")\n",
    "catalog = client.get_events(starttime=starttime, endtime=endtime, minmagnitude=7)\n",
    "```\n",
    "\n",
    "The data is returned in a catalog, which is a list-like container for events. To learn more about working with obspy, see the documentation and tutorials on the obspy [web site](chttps://docs.obspy.org/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ab70a",
   "metadata": {},
   "source": [
    "\n",
    "### Getting Seismic Data from DataSelect\n",
    "\n",
    "The `fdsnws-dataselect` service provides access to time series data for specified channels and time ranges. Dataselect implements the [FDSN web service specification](https://www.fdsn.org/webservices/).\n",
    "\n",
    "Data queries use SEED time series identifiers (network, station, location & channel) in addition to time ranges. Returned data formats include miniSEED, SAC zip, and GeoCSV.\n",
    "\n",
    "To create a request, the Dataselect API takes these parameters at a minimum:\n",
    "\n",
    "| parameters | examples | discussion | default |type |\n",
    "| ---------- | -------- | ---------- | ------- |-----|\n",
    "| start[time] |\t2010-02-27T06:30:00\t| Specifies the desired start-time for miniSEED data | | day/time |\n",
    "| end[time]\t| 2010-02-27T10:30:00 | Specify the end-time for the miniSEED data | | day/time | \n",
    "|net[work] | IU | Select one or more network codes. Accepts wildcards and lists. Can be SEED codes or data center-defined codes. | any | string |\n",
    "| sta[tion] | ANMO | Select one or more SEED station codes. Accepts wildcards and lists. | any| string |\n",
    "|loc[ation] |00 | Select one or more SEED location identifiers. Accepts wildcards and lists. Use -- for “Blank” location IDs (ID’s containing 2 spaces). | any | string |\n",
    "| cha[nnel] | BHZ | Select one or more SEED channel codes. Accepts wildcards and lists. | any | string |\n",
    "\n",
    "To download a file, we can use the `requests` package to send the HTTP request to the dataselect web service. As discussed in the previous section, the request must include an authorization token using the `get_token` function.\n",
    "\n",
    "The `download_data` function requires the query parameters required by the FDSN Web Service specification, and where to write the data. The function does several things. First, it requests an authorization token. Next, it creates a file name for the data. Finally, it sends the request to dataselect and writes the data to a file in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e1f63-bc44-49fd-8876-1e501731ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from earthscope_sdk import EarthScopeClient\n",
    "\n",
    "# SAGE archive\n",
    "URL = \"http://service.iris.edu/fdsnws/dataselect/1/query?\"\n",
    "\n",
    "# function to get authorization token \n",
    "def get_token():\n",
    "    \n",
    "    # refresh the token if it has expired\n",
    "    client.ctx.auth_flow.refresh_if_necessary()\n",
    "\n",
    "    token = client.ctx.auth_flow.access_token\n",
    "    \n",
    "    return token\n",
    "\n",
    "def download_data(params, data_directory):\n",
    "\n",
    "    # get authorization Bearer token\n",
    "    token = get_token()\n",
    "\n",
    "    # get year and day from string start time\n",
    "    start_date = datetime.strptime(params['start'], '%Y-%m-%dT%H:%M:%S')\n",
    "    year = start_date.year\n",
    "    day = start_date.day\n",
    "    \n",
    "    \n",
    "    # file name format: STATION.NETWORK.YEAR.DAYOFYEAR\n",
    "    file_name = \".\".join([params[\"sta\"], params[\"net\"],params['loc'],params['cha'], str(year), \"{:03d}\".format(day),'mseed'])\n",
    "    \n",
    "    \n",
    "    r = requests.get(URL, params=params, headers={\"authorization\": f\"Bearer {token}\"}, stream=True)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        # save the file\n",
    "        with open(Path(Path(data_directory) / file_name), 'wb') as f:\n",
    "            for data in r:\n",
    "                f.write(data)\n",
    "    else:\n",
    "        #problem occured\n",
    "        print(f\"failure: {r.status_code}, {r.reason}\")\n",
    "        \n",
    "\n",
    "# create client to get token\n",
    "client = EarthScopeClient()\n",
    "\n",
    "# create directory for data\n",
    "data_directory = \"./miniseed_data\"\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# parameters specifying the miniSEED file\n",
    "params = {\"net\" : 'IU',\n",
    "          \"sta\" : 'ANMO',\n",
    "          \"loc\" : '00',\n",
    "          \"cha\" : 'BHZ',\n",
    "          \"start\": '2010-02-27T06:30:00',\n",
    "          \"end\": '2010-02-27T10:30:00'}\n",
    "\n",
    "download_data(params, data_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729eab32",
   "metadata": {},
   "source": [
    "### Seismic Data in AWS S3\n",
    "\n",
    "Object storage in the cloud is a cost-effective way to hold and distribute very large collections of data. Objects consist of the data, metadata, and a unique identifier and are accessible through an application programming interface, or API. EarthScope uses Amazon Web Services' (AWS) Simple Storage Service, or S3, to store and distribute seismic and geodetic data.\n",
    "\n",
    "AWS S3 supports streaming data directly into memory. Streaming data is a significant advantage when analyzing large amounts of data because writing and reading data to and from a drive consumes the majority of time when performing an analysis. When data is streamed directly into memory, it is immediately available for processing.\n",
    "\n",
    "#### Buckets and Keys\n",
    "\n",
    "Objects in S3 are stored in containers called `buckets`. Each object is identified by a unique object identifier, or `keys`. Objects are accessed using a combination of the web service endpoint, a bucket name, and a key. The combination is called an ARN or an Amazon Resource Name. Unlike a hierarchical file system on your computer, S3 doesn't have directories; instead, it has prefixes, which act as filters that logically group data. Consider the following example, we can decipher the key:\n",
    "\n",
    "> s3:ncedc-pds/continuous_waveforms/BK/2022/2022.231/MERC.BK.HNZ.00.D.2022.231\n",
    "\n",
    "- s3 - service name\n",
    "- ncedc-pds - bucket name\n",
    "- continuous_waveforms - prefix\n",
    "- BK - (prefix) seismic network name \n",
    "- 2022 - (prefix) year \n",
    "- 2022.231 - (prefix) year and day of year\n",
    "- MERC.BK.HNZ.00.D.2022.231 - (key) station.network.channel.location.year.day of year\n",
    "\n",
    "Similar to a web service file URL, the ARN is used to request data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e373cc8",
   "metadata": {},
   "source": [
    "### S3 Buckets with Public Read Access\n",
    "\n",
    "S3 buckets can be configured for public read access, and you can access objects without providing credentials. The [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) Python package provides libraries for working with AWS services, including S3. Boto3 provides two methods for interacting with AWS services. The `client` method is a low-level and fine-grained interface that closely follows the AWS API for a service. The 'resource` method is a high-level interface that wraps the 'client` interface. AWS stopped development on the resource interface in `boto3` in 2023; for this reason, we will use the `client` interface when working with S3 resources.\n",
    "\n",
    "The following example reads a miniSEED file from the Northern California Earthquake Data Center (NCEDC). The trace data is for the 2014 Napa earthquake. GeoLab's default environment includes both `boto3` and `obspy` packages, and we can import them without installation. We establish the connection to S3 by creating a client that specifies that requests are unsigned. This means that the S3 bucket allows public access and does not require credentials. The client calls the `get_object` method with the bucket name and key for the miniSEED object. \n",
    "\n",
    "Instead of writing the data to a file (as we did using web services), the S3 client sends it to an in-memory binary stream that can be read by [`obspy`](https://docs.obspy.org/). Streaming the data to in-memory objects is more efficient than downloading and reading files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71065adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "# from io import BytesIO\n",
    "import io\n",
    "from obspy import read\n",
    "\n",
    "s3 = boto3.client('s3', config = Config(signature_version = UNSIGNED), region_name='us-west-2')\n",
    "\n",
    "BUCKET_NAME = 'ncedc-pds'\n",
    "KEY = 'continuous_waveforms/BK/2014/2014.236/PACP.BK.HHN.00.D.2014.236'\n",
    "\n",
    "response = s3.get_object(Bucket=BUCKET_NAME, Key=KEY)\n",
    "data_stream = io.BytesIO(response['Body'].read())\n",
    "\n",
    "# Parse with ObsPy\n",
    "st = read(data_stream)\n",
    "\n",
    "# Print the ObsPy Streams\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c43c58-49e0-43f2-b9bd-49b2456c5983",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "There are currently two options to get seismic data from EarthScope. You can use obspy or another third-party package, or use the dataselect service to select and sort event data in miniSEED. In the future, EarthScope will provide access to seismic data through AWS cloud services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c41243-75e2-41a0-99a8-339c4ad0613a",
   "metadata": {},
   "source": [
    "## [< Previous](./3_authorization.ipynb)&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[Next >](./5_seismic_exercise.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
