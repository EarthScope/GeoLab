{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5b6fbd",
   "metadata": {},
   "source": [
    "---\n",
    "# **Traditional vs Cloud: Geophysics with GeoLab**\n",
    "\n",
    "##### 🔶 This notebook demonstrates a side-by-side comparison between traditional geophysics research workflows and a modern, cloud-optimized approach through EarthScope’s GeoLab platform using GNSS 📡 data. By working through a typical data processing task in both environments, we highlight the practical differences in performance, scalability, and ease of use. GeoLab enables researchers to access open datasets, run compute-intensive workflows, and share reproducible analyses—all within a cloud-native ecosystem tailored to the needs of the geophysics community. \n",
    "\n",
    "*This demo is your hands-on guide to accessing and processing GNSS data—first through the traditional GAGE archive, and then using its cloud-based counterpart. In this notebook, you’ll follow a two-track journey:*\n",
    "\n",
    "***1.🚶 Traditional Serial Processing**\n",
    "   – Download RINEX files one by one to your local GeoLab environment*\n",
    "\n",
    "***2.☁️ Cloud-Optimized Workflow**\n",
    "   – Stream data directly from an AWS S3 bucket—no massive local downloads*\n",
    "\n",
    "*Before diving into the notebook, you need to be familiar with\n",
    "👉 [GAGE data archive](https://www.unavco.org/data/data.html) || 👉 [GNSS data products](https://www.unavco.org/data/gps-gnss/gps-gnss.html)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbaa7d",
   "metadata": {},
   "source": [
    "# 1️⃣ **Traditional Serial Processing**\n",
    "*In this section, we’ll explore the classic method of GNSS data handling which is downloading RINEX files to your GeoLab environment and processing them one by one.*\n",
    "\n",
    "##### 🔶 RINEX (Receiver Independent Exchange Format) is the open, ASCII-based standard for recording raw GNSS observations—packaging pseudorange, carrier-phase, navigation, and meteorological data into a human-readable, vendor-neutral text format. More about 👉 [RINEX](https://igs.org/wg/rinex/)\n",
    "\n",
    "*First, we’ll install the `GeoRINEX` package into a temporary environment using a bash script.* \n",
    "\n",
    "*For more details, 👉 [GeoRINEX GitHub repository](https://github.com/geospace-code/georinex/tree/main) || 👉 [Bash scripting inside notebook](https://stackoverflow.com/questions/58981651/how-to-run-a-shell-script-in-jupyter-notebook)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e76c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install georinex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8a4ac",
   "metadata": {},
   "source": [
    "*Now, we need to build a function for authenticating and downloading rinex file: **'rinex_download'.** This function will help you to pull files down from EarthScope's authenticated \"gage-data\" service in an automated script or pipeline. EarthScope’s “gage-data” portal requires `OAuth2` tokens; the SDK handles device-code flows, token caching, and token refreshing so you never have to copy-and-paste access tokens by hand. It allows you to stream large files without blowing out memory To learn more about how it works, 👉 [earthscope-sdk](https://docs.earthscope.org/projects/SDK/en/stable/content/usage.html)*\n",
    "\n",
    "*We will download from the EarthScope's GAGE data archive 👉*\n",
    "*[https://gage-data.earthscope.org/archive/gnss/rinex/obs/](https://gage-data.earthscope.org/archive/gnss/rinex/obs/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____ Authentication & Downloading function __________________________________________________\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from earthscope_sdk import AsyncEarthScopeClient\n",
    "\n",
    "async def download_rinex(station: str, year: int, doy: int, save_dir: str = \".\") -> Path:\n",
    "    \"\"\"\n",
    "    Download a RINEX .rnx.gz from EarthScope’s gage-data server.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    station : str\n",
    "        4-char station code, e.g. \"p057\"\n",
    "    year : int\n",
    "        Four-digit year, e.g. 2025\n",
    "    doy : int\n",
    "        Day-of-year (1–365 or 366)\n",
    "    save_dir : str\n",
    "        Directory where the file will be written (created if needed)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the downloaded .rnx.gz file\n",
    "    \"\"\"\n",
    "    # 1. Ensure output directory exists\n",
    "    out_dir = Path(save_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2. Construct the standard RINEX URL\n",
    "    filename = f\"{station}{doy:03d}0.24d.Z\"\n",
    "    url = f\"https://gage-data.earthscope.org/archive/gnss/rinex/obs/{year}/{doy:03d}/{filename}\"\n",
    "\n",
    "    # 3. Spin up the SDK’s async client (auto-auth, token refresh, retries)\n",
    "    async with AsyncEarthScopeClient() as client:\n",
    "        # Build & send via the SDK’s HTTPX client\n",
    "        req = client.ctx.httpx_client.build_request(\"GET\", url)\n",
    "        resp = await client.ctx.httpx_client.send(req)               # :contentReference[oaicite:0]{index=0}\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        # 4. Write out the file\n",
    "        out_path = out_dir / Path(url).name\n",
    "        out_path.write_bytes(resp.content)\n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849008dd",
   "metadata": {},
   "source": [
    "### **1. Single Download**\n",
    "\n",
    "*we will download GNSS data for a specific station, year, day and save it to a local directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4812b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports & Setup ─────────────────────────────────────────────────────────────\n",
    "# necessary packages for collecting, analyzing, plotting, and saving data\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import georinex as gr                    # GeoRINEX: convert RINEX -> xarray datasets (More on: https://github.com/geospace-code/georinex/blob/main/Readme_OBS.md)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ── Configuration of input ──────────────────────────────────────────────────────────────\n",
    "st_code   = 'p057'          # station code\n",
    "yr        = 2024            # the year of the observation data\n",
    "dy        = 10              # the day of the observation data\n",
    "save_dir  = 'rinex_data'    # output directory to save the rinex data\n",
    "\n",
    "# ── Single download ──────────────────────────────────────────────────────────────\n",
    "rinex_path = await download_rinex(st_code, yr, dy, save_dir=save_dir)\n",
    "print(\"Saved to\", rinex_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1504b39",
   "metadata": {},
   "source": [
    "### **2. Serial Download & Processing**\n",
    "\n",
    "*now, we will download GNSS data for multiple days in a year and calculate the mean value per day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Serial Download & Processing Loop ──────────────────────────────────────────\n",
    "doys = np.arange(1,11)      # doys = days of the year\n",
    "                            # setup to download RINEX data for first 10 days of the year\n",
    "\n",
    "# containers(empty arrays) to collect values in the loop\n",
    "snr_list = []\n",
    "dates    = []\n",
    "\n",
    "# Loop over day-of-year (doy)\n",
    "for doy in doys:\n",
    "    # 1️⃣ Download the file into rinex_path\n",
    "    rinex_path = await download_rinex(st_code, yr, doy, save_dir=save_dir)\n",
    "    print('_______')\n",
    "    print(\"Saved to\", rinex_path)\n",
    "\n",
    "    # 2️⃣ Load the downloaded RINEX file into an xarray dataset\n",
    "    obs = gr.load(rinex_path, use='G', meas=['S1'])  \n",
    "    #    - use='G' restricts to GPS satellites || RINEX can contain observations from multiple GNSS constellations: 'G' = GPS; 'R' = GLONASS; 'E' = Galileo, etc. ||\n",
    "    #    - meas=['S1'] loads only the L1C signal-to-noise ratio || RINEX files contain many different measurement 'meas' types for each satellite and epoch (C1, L1, S1, D1, etc.).\n",
    "\n",
    "    # 3️⃣ Compute the daily average SNR across all epochs & satellites\n",
    "    print(\"     Calculating mean ...\")\n",
    "    daily_mean_snr = obs['S1'].mean().values\n",
    "    snr_list.append(daily_mean_snr)\n",
    "\n",
    "    # 4️⃣ Record the corresponding calendar date\n",
    "    date = datetime(yr, 1, 1) + timedelta(days=int(doy - 1))\n",
    "    dates.append(date)\n",
    "\n",
    "    # 5️⃣ Clean up: delete the RINEX file to save disk space\n",
    "    os.remove(rinex_path)\n",
    "    print(\"Removed \", rinex_path)\n",
    "    print('_______')\n",
    "\n",
    "# ── Visualization ──────────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(dates, snr_list, marker='o', linestyle='-')\n",
    "plt.axhline(np.mean(snr_list), linestyle='--', label=f\"Mean SNR = {np.mean(snr_list):.2f}\")\n",
    "plt.title(f\"{len(dates)} Days of L1C SNR Averages (Serial Download & Processing))\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean L1C SNR\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e734e",
   "metadata": {},
   "source": [
    "*To recap, what the code snippet has done,*\n",
    "\n",
    "*Download GNSS observations    --> into `georinex` format     --> using `xarray` to slice/average*\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Comparison_satellite_navigation_orbits.svg/250px-Comparison_satellite_navigation_orbits.svg.png\" alt=\"GNSS\" width=\"200\"/>  <img src=\"https://raw.githubusercontent.com/asifashraf060/cloud-101-geolab/a3878c6335c72aab898d537011b3302c3c6c61f4/images/gr.png\" alt=\"GeoRINEX\" width=\"200\"/> <img src=\"https://camo.githubusercontent.com/2ee0e1a7be6338f330d4a2e69f86c9e545e9dc39e60fee9d52ee79b141b14d6b/68747470733a2f2f646f63732e7861727261792e6465762f656e2f737461626c652f5f7374617469632f5861727261795f4c6f676f5f5247425f46696e616c2e737667\" alt=\"Xarray\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a0ea6",
   "metadata": {},
   "source": [
    "# 2️⃣ **Cloud Optimized Workflow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01c0ad",
   "metadata": {},
   "source": [
    "Cloud computing provides on-demand access to flexible compute and storage resources over the internet.\n",
    "\n",
    "- **Compute**: Virtual machines (EC2) where your code runs; check out 👉 [EC2 documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)\n",
    "\n",
    "- **Storage**: Data lives on services like S3 (AWS); check out 👉 [S3 documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)\n",
    "\n",
    "- **Cluster**: A group of machines working together on large tasks\n",
    "\n",
    "These resources scale up/down in minutes, so you pay only for what you use. \n",
    "\n",
    "<img src=\"https://community.aws/_next/image?url=https%3A%2F%2Fassets.community.aws%2Fa%2F2YnihCpaNZkmFVuxyHKWrcxDSMT.png%3FimgSize%3D919x516&w=1080&q=75\" alt=\"awsEC2\" width=\"320\"/> \n",
    "<img src=\"https://i.ytimg.com/vi/ecv-19sYL3w/mqdefault.jpg\" alt=\"awsS3\" width=\"320\"/> <img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2022/06/09/Image-1a-VPC.png\" alt=\"awsS3\" width=\"315\"/> \n",
    "\n",
    "*First, lets tell the `AWS CLI` to start a Single Sign-On login flow using the “es-dev” profile—opening your browser to authenticate and caching temporary credentials for that profile.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sso login --profile es-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0b500",
   "metadata": {},
   "source": [
    "### **1. Cloud storage \"objects vs local files**\n",
    "Amazon S3 is an object storage service that stores data as objects\n",
    "\n",
    " 💠 \"bucket\" = top-level container\n",
    "\n",
    " 💠 \"object\" = data blob + metadata, addressed by a key (no hierarchical folders)\n",
    "\n",
    " 💠 \"file\"   = local disk item (you only get one if you download it)\n",
    "\n",
    " In essence, Amazon S3 keeps each file you upload as an object (data + metadata) inside a bucket you name. Every object is addressed by a unique key (the full path string you assign).\n",
    "\n",
    " ***Traditional storage vs Object Storage⤵️***\n",
    "\n",
    " <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsX-1XW2EVYCwz2IRH_2FQ.png\" alt=\"cloudStorage\" width=\"500\"/> \n",
    "\n",
    "To understand the **benifits for this object storage approach,**\n",
    "read the blog 👉 [EarthScope's cloud optimized data storage](https://www.earthscope.org/news/meet-tiledb-one-key-to-cloud-optimizing-our-data-archives/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b0279",
   "metadata": {},
   "source": [
    "##### **🚩🚩 📝 EXERCISE**\n",
    "\n",
    "From the blog, answer the following:\n",
    "\n",
    "**Q: What data format does EarthScope use to store its data in the cloud❓**\n",
    "\n",
    "*A: TileDB*\n",
    "\n",
    "**Q: Why is TileDB, when combined with object storage, well-suited for large-scale analysis❓**\n",
    "\n",
    "*A:* \n",
    "\n",
    "*1. Sparse Arrays: Efficiently represent large datasets with many empty or default values.*\n",
    "\n",
    "*2. Multi-Dimensional Querying: Enables fast access to data along one or more dimensions.*\n",
    "\n",
    "*3. Versioning Support: Each cell update is versioned, allowing retrieval by specific points in time.*\n",
    "\n",
    "*4. Parallel I/O: Supports concurrent read/write operations, ideal for multithreaded computing.*\n",
    "\n",
    "*5. Separation of Metadata: Metadata is managed via a relational database, improving scalability.*\n",
    "\n",
    "*6. Direct Data Access: Standardized array format allows direct loading into dataframes for immediate analysis.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165c412",
   "metadata": {},
   "source": [
    "##### Make URI (Uniform Resource Identifier) to access you GNSS file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1) Station & Cloud-storage identifiers\n",
    "# -----------------------------------------------------------------------------\n",
    "station_id  = \"P057\"                                                        # your GNSS station code\n",
    "bucket_name = \"repository-stage-us-east-2-mlmoghi3ooss/geolab-gnss-demo\"    # EarthScope’s S3 bucket (object store)\n",
    "object_key  = f\"{station_id}.tdb\"                                           # each .tdb is an immutable “object” in S3\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Build a “URI” to refer to your object\n",
    "# -----------------------------------------------------------------------------\n",
    "s3_uri = f\"s3://{bucket_name}/{object_key}\" # This is EarthScope's bucket-centric architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99e3e5",
   "metadata": {},
   "source": [
    "##### **🚩🚩 📝 EXERCISE**\n",
    "\n",
    "**Q: Explain the input parameters in the previous cell in terms of the object storage approach in AWS S3❓**\n",
    "\n",
    "*A: bucket_name - bucket; object_key = object; station_id = file*\n",
    "\n",
    "**Q: Compare the directory in HPC/Local vs the Cloud for the GNSS file❓**\n",
    "\n",
    "*A: In traditional HPC you think “file /data/P057.tdb.” In the cloud it’s “object key P057.tdb in bucket geolab-gnss-demo.” There are no real directories on S3—just keys.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b595ad8",
   "metadata": {},
   "source": [
    "### **2. ARCO-style data concept**\n",
    "*Each object is immutable and carries metadata (upload timestamp, tags). You refer to it by bucket+key, and you can attach custom metadata (e.g. station, observation type).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Time‐window for your download\n",
    "# -----------------------------------------------------------------------------\n",
    "start_iso      = \"2024-05-11\"         # ISO format is standard in ARCO specs\n",
    "duration_hours = 12\n",
    "\n",
    "start_dt = datetime.fromisoformat(start_iso)\n",
    "end_dt   = start_dt + timedelta(hours=duration_hours)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Observation parameters (per ARCO data dictionary)\n",
    "# -----------------------------------------------------------------------------\n",
    "# – constellation: 0=GPS, 1=GLONASS, 2=Galileo, …\n",
    "# – obs_code:      numeric GNSS observation type\n",
    "constellation_code = 0    # GPS\n",
    "raw_obs_code       = 12611  # e.g. “GPS L1 C/A pseudorange” per ARCO spec\n",
    "\n",
    "# Some client libraries expect obs_code packed as 2‐byte big-endian:\n",
    "obs_code_bytes = raw_obs_code.to_bytes(2, \"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101aeff",
   "metadata": {},
   "source": [
    "### **3. Access EarthScope's data lake in AWS S3**\n",
    "\n",
    "*We will use `boto3` to start an AWS session, which is a friendly, Pythonic way to talk to AWS services.*\n",
    "\n",
    "*It is the official AWS SDK for Python.*\n",
    "\n",
    "*It handles all the gritty details of signing requests, managing credentials, and retrying fallen network calls, so you can focus on your data rather than on AWS plumbing.*\n",
    "*To learn more  👉  [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4) Setup AWS credentials & TileDB config\n",
    "# ============================================================================\n",
    "import boto3\n",
    "# ———————————————————————————————————————————————————————————————\n",
    "# 4a) Load your AWS credentials via boto3 Session\n",
    "#     This will look for ~/.aws/credentials or $AWS_PROFILE\n",
    "# ———————————————————————————————————————————————————————————————\n",
    "session    = boto3.Session(profile_name=\"es-dev\")\n",
    "creds      = session.get_credentials().get_frozen_credentials()  # Returns a “credentials provider” object and converts that into an immutable snapshot of raw values\n",
    "# ———————————————————————————————————————————————————————————————\n",
    "# 4b) Build a config dict for TileDB’s S3 virtual filesystem (VFS)\n",
    "#     so TileDB knows how to authenticate & parallelize I/O.\n",
    "# ———————————————————————————————————————————————————————————————\n",
    "tdb_config = {\n",
    "    # Which AWS region your bucket lives in\n",
    "    \"vfs.s3.region\":                \"us-east-2\",\n",
    "    # How many concurrent connections to use when talking to S3\n",
    "    \"sm.io_concurrency_level\":      12,\n",
    "    # Your access keys (pulled from the frozen credentials snapshot)\n",
    "    \"vfs.s3.aws_access_key_id\":     creds.access_key,\n",
    "    \"vfs.s3.aws_secret_access_key\": creds.secret_key,\n",
    "    # If you’re using temporary session tokens, include this too\n",
    "    \"vfs.s3.aws_session_token\":     creds.token,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afdc36",
   "metadata": {},
   "source": [
    "### **4. Object read (streaming)**\n",
    "- Open the array directly on S3\n",
    "- Data flows over the network into memory\n",
    "- No local file is ever created\n",
    "- This is completely opposite of file download, which is full copy\n",
    "\n",
    "*In the following cell you will open a remote `tiledb` array on S3, load the specified time‐range slice into a pandas DataFrame.*\n",
    "\n",
    "*We will do it in two ways,*\n",
    "\n",
    "*Option 1: One-shot slice --> single open & single request, fewer S3 round trips, lower per request overhead*\n",
    "\n",
    "*Option 2: Incremental daily chunks --> repeated open & repeated request, higher S3 round trips, stream small slices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledb\n",
    "# ============================================================================\n",
    "# 5) Object read (streaming)\n",
    "#    — Good for pipelines that parse on-the-fly, without touching disk\n",
    "# ============================================================================\n",
    "\n",
    "# 5a) Option 1: One-shot slice\n",
    "# Open the TileDB array in “read” mode with your AWS creds baked in\n",
    "with tiledb.open(s3_uri, mode=\"r\", config=tdb_config) as A:\n",
    "    A.upgrade_version()  # Ensure compatibility if schema has changed\n",
    "\n",
    "    # Slice out exactly the rows you need; TileDB streams them from S3\n",
    "    df_stream = A.df[\n",
    "        unicode_time_millis(start_dt)  : unicode_time_millis(end_dt),\n",
    "        :  # keep all other dimensions\n",
    "    ]\n",
    "# 5b) Option 2: Incremental daily chunks\n",
    "for doy in np.arange(1,51):\n",
    "    start=datetime(year, 1, 1) + timedelta(days=int(doy - 1))\n",
    "    date_li+=[start]\n",
    "    end=start+timedelta(days=1)\n",
    "    start=unix_time_millis(start)\n",
    "    end=unix_time_millis(end)\n",
    "  \n",
    "    with tiledb.open(s3_uri, mode=\"r\", config=tdb_config,) as A:\n",
    "        df=A.df[slice(int(start), int(end)),:]['snr']\n",
    "        snr_li+=[df.mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5e18d",
   "metadata": {},
   "source": [
    "##### **🚩🚩 📝 EXERCISE**\n",
    "\n",
    "**Q: How option 1 and 2 will work for the following use cases/scenarios❓**\n",
    "\n",
    "A:\n",
    "\n",
    "| Scenario                                      | Option 1: Single-shot slice                                                                | Option 2: Incremental daily chunks                                           |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- |\n",
    "| **One-off analysis over a continuous window** | ✔️ Fetch entire `start → end` range in one call<br>– Fewer S3 round-trips<br>– Simple code | ❌ Too many repeated opens/reads                                              |\n",
    "| **Small-to-medium window**                    | ✔️ Efficient bulk read<br>– Minimal overhead                                               | ✔️ Works but incurs extra latency per chunk                                  |\n",
    "| **Very large window (days/weeks/months)**     | ❌ May exhaust RAM if you load too much data at once                                        | ✔️ Streams day by day<br>– Caps memory use per slice                         |\n",
    "| **Fine-grained per-day metrics**              | ❌ You’d need to slice & loop locally after download                                        | ✔️ Computes each day’s metric on-the-fly<br>– No large in-memory DataFrame   |\n",
    "| **Low-latency needs (few requests)**          | ✔️ Single network hit<br>– Faster end-to-end for moderate windows                          | ❌ Many handshakes add up                                                     |\n",
    "| **Minimal local storage footprint**           | ✔️ No disk writes                                                                          | ✔️ Only per-chunk in-memory; no full download                                |\n",
    "| **Reuse same data repeatedly**                | ✔️ Good if you only need one contiguous pass                                               | ❌ Re-streaming same ranges wastes network and time                           |\n",
    "| **Disk-based tools require a file path**      | ❌ Not applicable (no local file)                                                           | ❌ Streaming only; consider full download first (see “file download” pattern) |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6e7f5",
   "metadata": {},
   "source": [
    "##### **🚩🚩 📝 FINAL EXERCISE 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩**\n",
    "**Q: What is the difference in total time taken to complete the download and processing steps between the traditional workflow and the cloud-optimized workflow?❓**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b0ca8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
