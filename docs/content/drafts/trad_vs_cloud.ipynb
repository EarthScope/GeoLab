{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5b6fbd",
   "metadata": {},
   "source": [
    "---\n",
    "# **Traditional vs Cloud: Geophysics with GeoLab**\n",
    "\n",
    "##### ğŸ”¶ This notebook demonstrates a side-by-side comparison between traditional geophysics research workflows and a modern, cloud-optimized approach through EarthScopeâ€™s GeoLab platform using GNSS ğŸ“¡ data. By working through a typical data processing task in both environments, we highlight the practical differences in performance, scalability, and ease of use. GeoLab enables researchers to access open datasets, run compute-intensive workflows, and share reproducible analysesâ€”all within a cloud-native ecosystem tailored to the needs of the geophysics community. \n",
    "\n",
    "*This demo is your hands-on guide to accessing and processing GNSS dataâ€”first through the traditional GAGE archive, and then using its cloud-based counterpart. In this notebook, youâ€™ll follow a two-track journey:*\n",
    "\n",
    "***1.ğŸš¶ Traditional Serial Processing**\n",
    "   â€“ Download RINEX files one by one to your local GeoLab environment*\n",
    "\n",
    "***2.â˜ï¸ Cloud-Optimized Workflow**\n",
    "   â€“ Stream data directly from an AWS S3 bucketâ€”no massive local downloads*\n",
    "\n",
    "*Before diving into the notebook, you need to be familiar with\n",
    "ğŸ‘‰ [GAGE data archive](https://www.unavco.org/data/data.html) || ğŸ‘‰ [GNSS data products](https://www.unavco.org/data/gps-gnss/gps-gnss.html)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbaa7d",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ **Traditional Serial Processing**\n",
    "*In this section, weâ€™ll explore the classic method of GNSS data handling which is downloading RINEX files to your GeoLab environment and processing them one by one.*\n",
    "\n",
    "##### ğŸ”¶ RINEX (Receiver Independent Exchange Format) is the open, ASCII-based standard for recording raw GNSS observationsâ€”packaging pseudorange, carrier-phase, navigation, and meteorological data into a human-readable, vendor-neutral text format. More about ğŸ‘‰ [RINEX](https://igs.org/wg/rinex/)\n",
    "\n",
    "*First, weâ€™ll install the `GeoRINEX` package into a temporary environment using a bash script.* \n",
    "\n",
    "*For more details, ğŸ‘‰ [GeoRINEX GitHub repository](https://github.com/geospace-code/georinex/tree/main) || ğŸ‘‰ [Bash scripting inside notebook](https://stackoverflow.com/questions/58981651/how-to-run-a-shell-script-in-jupyter-notebook)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e76c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install georinex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8a4ac",
   "metadata": {},
   "source": [
    "*Now, we need to build a function for authenticating and downloading rinex file: **'rinex_download'.** This function will help you to pull files down from EarthScope's authenticated \"gage-data\" service in an automated script or pipeline. EarthScopeâ€™s â€œgage-dataâ€ portal requires `OAuth2` tokens; the SDK handles device-code flows, token caching, and token refreshing so you never have to copy-and-paste access tokens by hand. It allows you to stream large files without blowing out memory To learn more about how it works, ğŸ‘‰ [earthscope-sdk](https://docs.earthscope.org/projects/SDK/en/stable/content/usage.html)*\n",
    "\n",
    "*We will download from the EarthScope's GAGE data archive ğŸ‘‰*\n",
    "*[https://gage-data.earthscope.org/archive/gnss/rinex/obs/](https://gage-data.earthscope.org/archive/gnss/rinex/obs/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____ Authentication & Downloading function __________________________________________________\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from earthscope_sdk import AsyncEarthScopeClient\n",
    "\n",
    "async def download_rinex(station: str, year: int, doy: int, save_dir: str = \".\") -> Path:\n",
    "    \"\"\"\n",
    "    Download a RINEX .rnx.gz from EarthScopeâ€™s gage-data server.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    station : str\n",
    "        4-char station code, e.g. \"p057\"\n",
    "    year : int\n",
    "        Four-digit year, e.g. 2025\n",
    "    doy : int\n",
    "        Day-of-year (1â€“365 or 366)\n",
    "    save_dir : str\n",
    "        Directory where the file will be written (created if needed)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the downloaded .rnx.gz file\n",
    "    \"\"\"\n",
    "    # 1. Ensure output directory exists\n",
    "    out_dir = Path(save_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2. Construct the standard RINEX URL\n",
    "    filename = f\"{station}{doy:03d}0.24d.Z\"\n",
    "    url = f\"https://gage-data.earthscope.org/archive/gnss/rinex/obs/{year}/{doy:03d}/{filename}\"\n",
    "\n",
    "    # 3. Spin up the SDKâ€™s async client (auto-auth, token refresh, retries)\n",
    "    async with AsyncEarthScopeClient() as client:\n",
    "        # Build & send via the SDKâ€™s HTTPX client\n",
    "        req = client.ctx.httpx_client.build_request(\"GET\", url)\n",
    "        resp = await client.ctx.httpx_client.send(req)               # :contentReference[oaicite:0]{index=0}\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        # 4. Write out the file\n",
    "        out_path = out_dir / Path(url).name\n",
    "        out_path.write_bytes(resp.content)\n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849008dd",
   "metadata": {},
   "source": [
    "### **1. Single Download**\n",
    "\n",
    "*we will download GNSS data for a specific station, year, day and save it to a local directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4812b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Imports & Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# necessary packages for collecting, analyzing, plotting, and saving data\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import georinex as gr                    # GeoRINEX: convert RINEX -> xarray datasets (More on: https://github.com/geospace-code/georinex/blob/main/Readme_OBS.md)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# â”€â”€ Configuration of input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "st_code   = 'p057'          # station code\n",
    "yr        = 2024            # the year of the observation data\n",
    "dy        = 10              # the day of the observation data\n",
    "save_dir  = 'rinex_data'    # output directory to save the rinex data\n",
    "\n",
    "# â”€â”€ Single download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rinex_path = await download_rinex(st_code, yr, dy, save_dir=save_dir)\n",
    "print(\"Saved to\", rinex_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1504b39",
   "metadata": {},
   "source": [
    "### **2. Serial Download & Processing**\n",
    "\n",
    "*now, we will download GNSS data for multiple days in a year and calculate the mean value per day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Serial Download & Processing Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "doys = np.arange(1,11)      # doys = days of the year\n",
    "                            # setup to download RINEX data for first 10 days of the year\n",
    "\n",
    "# containers(empty arrays) to collect values in the loop\n",
    "snr_list = []\n",
    "dates    = []\n",
    "\n",
    "# Loop over day-of-year (doy)\n",
    "for doy in doys:\n",
    "    # 1ï¸âƒ£ Download the file into rinex_path\n",
    "    rinex_path = await download_rinex(st_code, yr, doy, save_dir=save_dir)\n",
    "    print('_______')\n",
    "    print(\"Saved to\", rinex_path)\n",
    "\n",
    "    # 2ï¸âƒ£ Load the downloaded RINEX file into an xarray dataset\n",
    "    obs = gr.load(rinex_path, use='G', meas=['S1'])  \n",
    "    #    - use='G' restricts to GPS satellites || RINEX can contain observations from multiple GNSS constellations: 'G' = GPS; 'R' = GLONASS; 'E' = Galileo, etc. ||\n",
    "    #    - meas=['S1'] loads only the L1C signal-to-noise ratio || RINEX files contain many different measurement 'meas' types for each satellite and epoch (C1, L1, S1, D1, etc.).\n",
    "\n",
    "    # 3ï¸âƒ£ Compute the daily average SNR across all epochs & satellites\n",
    "    print(\"     Calculating mean ...\")\n",
    "    daily_mean_snr = obs['S1'].mean().values\n",
    "    snr_list.append(daily_mean_snr)\n",
    "\n",
    "    # 4ï¸âƒ£ Record the corresponding calendar date\n",
    "    date = datetime(yr, 1, 1) + timedelta(days=int(doy - 1))\n",
    "    dates.append(date)\n",
    "\n",
    "    # 5ï¸âƒ£ Clean up: delete the RINEX file to save disk space\n",
    "    os.remove(rinex_path)\n",
    "    print(\"Removed \", rinex_path)\n",
    "    print('_______')\n",
    "\n",
    "# â”€â”€ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(dates, snr_list, marker='o', linestyle='-')\n",
    "plt.axhline(np.mean(snr_list), linestyle='--', label=f\"Mean SNR = {np.mean(snr_list):.2f}\")\n",
    "plt.title(f\"{len(dates)} Days of L1C SNR Averages (Serial Download & Processing))\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Mean L1C SNR\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e734e",
   "metadata": {},
   "source": [
    "*To recap, what the code snippet has done,*\n",
    "\n",
    "*Download GNSS observations    --> into `georinex` format     --> using `xarray` to slice/average*\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Comparison_satellite_navigation_orbits.svg/250px-Comparison_satellite_navigation_orbits.svg.png\" alt=\"GNSS\" width=\"200\"/>  <img src=\"https://raw.githubusercontent.com/asifashraf060/cloud-101-geolab/a3878c6335c72aab898d537011b3302c3c6c61f4/images/gr.png\" alt=\"GeoRINEX\" width=\"200\"/> <img src=\"https://camo.githubusercontent.com/2ee0e1a7be6338f330d4a2e69f86c9e545e9dc39e60fee9d52ee79b141b14d6b/68747470733a2f2f646f63732e7861727261792e6465762f656e2f737461626c652f5f7374617469632f5861727261795f4c6f676f5f5247425f46696e616c2e737667\" alt=\"Xarray\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a0ea6",
   "metadata": {},
   "source": [
    "# 2ï¸âƒ£ **Cloud Optimized Workflow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01c0ad",
   "metadata": {},
   "source": [
    "Cloud computing provides on-demand access to flexible compute and storage resources over the internet.\n",
    "\n",
    "- **Compute**: Virtual machines (EC2) where your code runs; check out ğŸ‘‰ [EC2 documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)\n",
    "\n",
    "- **Storage**: Data lives on services like S3 (AWS); check out ğŸ‘‰ [S3 documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)\n",
    "\n",
    "- **Cluster**: A group of machines working together on large tasks\n",
    "\n",
    "These resources scale up/down in minutes, so you pay only for what you use. \n",
    "\n",
    "<img src=\"https://community.aws/_next/image?url=https%3A%2F%2Fassets.community.aws%2Fa%2F2YnihCpaNZkmFVuxyHKWrcxDSMT.png%3FimgSize%3D919x516&w=1080&q=75\" alt=\"awsEC2\" width=\"320\"/> \n",
    "<img src=\"https://i.ytimg.com/vi/ecv-19sYL3w/mqdefault.jpg\" alt=\"awsS3\" width=\"320\"/> <img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2022/06/09/Image-1a-VPC.png\" alt=\"awsS3\" width=\"315\"/> \n",
    "\n",
    "*First, lets tell the `AWS CLI` to start a Single Sign-On login flow using the â€œes-devâ€ profileâ€”opening your browser to authenticate and caching temporary credentials for that profile.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sso login --profile es-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0b500",
   "metadata": {},
   "source": [
    "### **1. Cloud storage \"objects vs local files**\n",
    "Amazon S3 is an object storage service that stores data as objects\n",
    "\n",
    " ğŸ’  \"bucket\" = top-level container\n",
    "\n",
    " ğŸ’  \"object\" = data blob + metadata, addressed by a key (no hierarchical folders)\n",
    "\n",
    " ğŸ’  \"file\"   = local disk item (you only get one if you download it)\n",
    "\n",
    " In essence, AmazonÂ S3 keeps each file you upload as an object (data + metadata) inside a bucket you name. Every object is addressed by a unique key (the full path string you assign).\n",
    "\n",
    " ***Traditional storage vs Object Storageâ¤µï¸***\n",
    "\n",
    " <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsX-1XW2EVYCwz2IRH_2FQ.png\" alt=\"cloudStorage\" width=\"500\"/> \n",
    "\n",
    "To understand the **benifits for this object storage approach,**\n",
    "read the blog ğŸ‘‰ [EarthScope's cloud optimized data storage](https://www.earthscope.org/news/meet-tiledb-one-key-to-cloud-optimizing-our-data-archives/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b0279",
   "metadata": {},
   "source": [
    "##### **ğŸš©ğŸš© ğŸ“ EXERCISE**\n",
    "\n",
    "From the blog, answer the following:\n",
    "\n",
    "**Q: What data format does EarthScope use to store its data in the cloudâ“**\n",
    "\n",
    "*A: TileDB*\n",
    "\n",
    "**Q: Why is TileDB, when combined with object storage, well-suited for large-scale analysisâ“**\n",
    "\n",
    "*A:* \n",
    "\n",
    "*1. Sparse Arrays: Efficiently represent large datasets with many empty or default values.*\n",
    "\n",
    "*2. Multi-Dimensional Querying: Enables fast access to data along one or more dimensions.*\n",
    "\n",
    "*3. Versioning Support: Each cell update is versioned, allowing retrieval by specific points in time.*\n",
    "\n",
    "*4. Parallel I/O: Supports concurrent read/write operations, ideal for multithreaded computing.*\n",
    "\n",
    "*5. Separation of Metadata: Metadata is managed via a relational database, improving scalability.*\n",
    "\n",
    "*6. Direct Data Access: Standardized array format allows direct loading into dataframes for immediate analysis.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165c412",
   "metadata": {},
   "source": [
    "##### Make URI (Uniform Resource Identifier) to access you GNSS file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1) Station & Cloud-storage identifiers\n",
    "# -----------------------------------------------------------------------------\n",
    "station_id  = \"P057\"                                                        # your GNSS station code\n",
    "bucket_name = \"repository-stage-us-east-2-mlmoghi3ooss/geolab-gnss-demo\"    # EarthScopeâ€™s S3 bucket (object store)\n",
    "object_key  = f\"{station_id}.tdb\"                                           # each .tdb is an immutable â€œobjectâ€ in S3\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Build a â€œURIâ€ to refer to your object\n",
    "# -----------------------------------------------------------------------------\n",
    "s3_uri = f\"s3://{bucket_name}/{object_key}\" # This is EarthScope's bucket-centric architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99e3e5",
   "metadata": {},
   "source": [
    "##### **ğŸš©ğŸš© ğŸ“ EXERCISE**\n",
    "\n",
    "**Q: Explain the input parameters in the previous cell in terms of the object storage approach in AWS S3â“**\n",
    "\n",
    "*A: bucket_name - bucket; object_key = object; station_id = file*\n",
    "\n",
    "**Q: Compare the directory in HPC/Local vs the Cloud for the GNSS fileâ“**\n",
    "\n",
    "*A: In traditional HPC you think â€œfile /data/P057.tdb.â€ In the cloud itâ€™s â€œobject key P057.tdb in bucket geolab-gnss-demo.â€ There are no real directories on S3â€”just keys.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b595ad8",
   "metadata": {},
   "source": [
    "### **2. ARCO-style data concept**\n",
    "*Each object is immutable and carries metadata (upload timestamp, tags). You refer to it by bucket+key, and you can attach custom metadata (e.g. station, observation type).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Timeâ€window for your download\n",
    "# -----------------------------------------------------------------------------\n",
    "start_iso      = \"2024-05-11\"         # ISO format is standard in ARCO specs\n",
    "duration_hours = 12\n",
    "\n",
    "start_dt = datetime.fromisoformat(start_iso)\n",
    "end_dt   = start_dt + timedelta(hours=duration_hours)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Observation parameters (per ARCO data dictionary)\n",
    "# -----------------------------------------------------------------------------\n",
    "# â€“ constellation: 0=GPS, 1=GLONASS, 2=Galileo, â€¦\n",
    "# â€“ obs_code:      numeric GNSS observation type\n",
    "constellation_code = 0    # GPS\n",
    "raw_obs_code       = 12611  # e.g. â€œGPS L1 C/A pseudorangeâ€ per ARCO spec\n",
    "\n",
    "# Some client libraries expect obs_code packed as 2â€byte big-endian:\n",
    "obs_code_bytes = raw_obs_code.to_bytes(2, \"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101aeff",
   "metadata": {},
   "source": [
    "### **3. Access EarthScope's data lake in AWS S3**\n",
    "\n",
    "*We will use `boto3` to start an AWS session, which is a friendly, Pythonic way to talk to AWS services.*\n",
    "\n",
    "*It is the official AWS SDK for Python.*\n",
    "\n",
    "*It handles all the gritty details of signing requests, managing credentials, and retrying fallen network calls, so you can focus on your data rather than on AWS plumbing.*\n",
    "*To learn more  ğŸ‘‰  [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4) Setup AWS credentials & TileDB config\n",
    "# ============================================================================\n",
    "import boto3\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# 4a) Load your AWS credentials via boto3 Session\n",
    "#     This will look for ~/.aws/credentials or $AWS_PROFILE\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "session    = boto3.Session(profile_name=\"es-dev\")\n",
    "creds      = session.get_credentials().get_frozen_credentials()  # Returns a â€œcredentials providerâ€ object and converts that into an immutable snapshot of raw values\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# 4b) Build a config dict for TileDBâ€™s S3 virtual filesystem (VFS)\n",
    "#     so TileDB knows how to authenticate & parallelize I/O.\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "tdb_config = {\n",
    "    # Which AWS region your bucket lives in\n",
    "    \"vfs.s3.region\":                \"us-east-2\",\n",
    "    # How many concurrent connections to use when talking to S3\n",
    "    \"sm.io_concurrency_level\":      12,\n",
    "    # Your access keys (pulled from the frozen credentials snapshot)\n",
    "    \"vfs.s3.aws_access_key_id\":     creds.access_key,\n",
    "    \"vfs.s3.aws_secret_access_key\": creds.secret_key,\n",
    "    # If youâ€™re using temporary session tokens, include this too\n",
    "    \"vfs.s3.aws_session_token\":     creds.token,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afdc36",
   "metadata": {},
   "source": [
    "### **4. Object read (streaming)**\n",
    "- Open the array directly on S3\n",
    "- Data flows over the network into memory\n",
    "- No local file is ever created\n",
    "- This is completely opposite of file download, which is full copy\n",
    "\n",
    "*In the following cell you will open a remote `tiledb` array on S3, load the specified timeâ€range slice into a pandas DataFrame.*\n",
    "\n",
    "*We will do it in two ways,*\n",
    "\n",
    "*Option 1: One-shot slice --> single open & single request, fewer S3 round trips, lower per request overhead*\n",
    "\n",
    "*Option 2: Incremental daily chunks --> repeated open & repeated request, higher S3 round trips, stream small slices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledb\n",
    "# ============================================================================\n",
    "# 5) Object read (streaming)\n",
    "#    â€” Good for pipelines that parse on-the-fly, without touching disk\n",
    "# ============================================================================\n",
    "\n",
    "# 5a) Option 1: One-shot slice\n",
    "# Open the TileDB array in â€œreadâ€ mode with your AWS creds baked in\n",
    "with tiledb.open(s3_uri, mode=\"r\", config=tdb_config) as A:\n",
    "    A.upgrade_version()  # Ensure compatibility if schema has changed\n",
    "\n",
    "    # Slice out exactly the rows you need; TileDB streams them from S3\n",
    "    df_stream = A.df[\n",
    "        unicode_time_millis(start_dt)  : unicode_time_millis(end_dt),\n",
    "        :  # keep all other dimensions\n",
    "    ]\n",
    "# 5b) Option 2: Incremental daily chunks\n",
    "for doy in np.arange(1,51):\n",
    "    start=datetime(year, 1, 1) + timedelta(days=int(doy - 1))\n",
    "    date_li+=[start]\n",
    "    end=start+timedelta(days=1)\n",
    "    start=unix_time_millis(start)\n",
    "    end=unix_time_millis(end)\n",
    "  \n",
    "    with tiledb.open(s3_uri, mode=\"r\", config=tdb_config,) as A:\n",
    "        df=A.df[slice(int(start), int(end)),:]['snr']\n",
    "        snr_li+=[df.mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5e18d",
   "metadata": {},
   "source": [
    "##### **ğŸš©ğŸš© ğŸ“ EXERCISE**\n",
    "\n",
    "**Q: How option 1 and 2 will work for the following use cases/scenariosâ“**\n",
    "\n",
    "A:\n",
    "\n",
    "| Scenario                                      | Option 1: Single-shot slice                                                                | Option 2: Incremental daily chunks                                           |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- |\n",
    "| **One-off analysis over a continuous window** | âœ”ï¸ Fetch entire `start â†’ end` range in one call<br>â€“ Fewer S3 round-trips<br>â€“ Simple code | âŒ Too many repeated opens/reads                                              |\n",
    "| **Small-to-medium window**                    | âœ”ï¸ Efficient bulk read<br>â€“ Minimal overhead                                               | âœ”ï¸ Works but incurs extra latency per chunk                                  |\n",
    "| **Very large window (days/weeks/months)**     | âŒ May exhaust RAM if you load too much data at once                                        | âœ”ï¸ Streams day by day<br>â€“ Caps memory use per slice                         |\n",
    "| **Fine-grained per-day metrics**              | âŒ Youâ€™d need to slice & loop locally after download                                        | âœ”ï¸ Computes each dayâ€™s metric on-the-fly<br>â€“ No large in-memory DataFrame   |\n",
    "| **Low-latency needs (few requests)**          | âœ”ï¸ Single network hit<br>â€“ Faster end-to-end for moderate windows                          | âŒ Many handshakes add up                                                     |\n",
    "| **Minimal local storage footprint**           | âœ”ï¸ No disk writes                                                                          | âœ”ï¸ Only per-chunk in-memory; no full download                                |\n",
    "| **Reuse same data repeatedly**                | âœ”ï¸ Good if you only need one contiguous pass                                               | âŒ Re-streaming same ranges wastes network and time                           |\n",
    "| **Disk-based tools require a file path**      | âŒ Not applicable (no local file)                                                           | âŒ Streaming only; consider full download first (see â€œfile downloadâ€ pattern) |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6e7f5",
   "metadata": {},
   "source": [
    "##### **ğŸš©ğŸš© ğŸ“ FINAL EXERCISE ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©**\n",
    "**Q: What is the difference in total time taken to complete the download and processing steps between the traditional workflow and the cloud-optimized workflow?â“**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b0ca8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
